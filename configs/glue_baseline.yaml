
sft_config:
  per_device_train_batch_size: 4
  report_to: none
  num_train_epochs: 3.0
  # report_to="wandb",  # this tells the Trainer to log the metrics to W&B
  save_strategy: epoch
  optim: adamw_torch
  adam_epsilon: 0.00000001
  # lr_scheduler_type options: [linear_with_warmup, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup, inverse_sqrt]
  lr_scheduler_type: linear 
  bf16: True
  learning_rate: 0.00005
  warmup_ratio: 0.1
  
